# ðŸŽ¬ Gemini 3 Live API - Real-Time Multimodal AI

## Overview
Gemini 3 Live represents a breakthrough in real-time, multimodal AI interaction. It can **see, hear, and speak simultaneously** with sub-second latency, making it perfect for building truly conversational AI experiences.

## Key Capabilities

### ðŸŽ¯ Multimodal Processing
- **Visual**: Real-time video/screen capture analysis
- **Audio**: Native speech understanding with emotional nuance
- **Text**: Standard text input/output
- **Simultaneous**: Process all modalities at once

### âš¡ Real-Time Features
- **WebSocket Streaming**: Bidirectional, low-latency communication
- **Voice Activity Detection**: Automatic turn-taking
- **Interruption Support**: Natural conversation flow
- **Session Memory**: Context retention throughout interaction

### ðŸŽ›ï¸ Advanced Controls
- **Thinking Level**: Control reasoning depth (Low/Medium/High)
- **Media Resolution**: Balance quality vs. speed
- **Grounding**: Real-time Google Search integration
- **Tool Use**: Function calling and code execution

## Quick Start Examples

### Example 1: Basic Text Conversation
```typescript
import { GoogleGenerativeAI } from '@google/generative-ai';

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_GEMINI_API_KEY!);
const model = genAI.getGenerativeModel({
  model: "gemini-3.0-pro",
  generationConfig: {
    temperature: 0.7,
    maxOutputTokens: 2000,
  }
});

const result = await model.generateContent("Explain quantum computing");
console.log(result.response.text());
```

### Example 2: Live WebSocket Session
```typescript
import WebSocket from 'ws';
import { GeminiLiveClient } from './gemini-live-client';

const client = new GeminiLiveClient({
  apiKey: process.env.GOOGLE_GEMINI_API_KEY!,
  model: 'gemini-3.0-live',
  thinkingLevel: 'medium',
  mediaResolution: 'high',
});

// Start live session
await client.connect();

// Send text
await client.sendText("Hello, can you see my screen?");

// Stream video frames
await client.streamVideo(videoFrameBuffer);

// Send audio
await client.streamAudio(audioBuffer);

// Listen for responses
client.on('response', (response) => {
  if (response.audio) {
    playAudio(response.audio); // Play generated speech
  }
  if (response.text) {
    console.log('Gemini:', response.text);
  }
});
```

### Example 3: Screen-to-Code (Vibe Coding)
```typescript
import { GeminiVibeCoder } from './vibe-coder';

const coder = new GeminiVibeCoder({
  thinkingLevel: 'high', // Deep think for complex UI
});

// Capture screen and generate code
const screenshot = await captureScreen();
const code = await coder.screenToCode(screenshot, {
  framework: 'react',
  styling: 'tailwind',
  typescript: true,
});

console.log(code);
// Output: Complete React component matching the UI
```

### Example 4: Multimodal Agent with Tools
```typescript
import { GeminiAgent } from './gemini-agent';

const agent = new GeminiAgent({
  model: 'gemini-3.0-pro',
  thinkingLevel: 'high',
  tools: [
    {
      name: 'search',
      description: 'Search the web',
      execute: async (query) => googleSearch(query),
    },
    {
      name: 'calculate',
      description: 'Perform calculations',
      execute: async (expression) => eval(expression),
    },
    {
      name: 'code_execute',
      description: 'Run Python code',
      execute: async (code) => runPython(code),
    },
  ],
});

// Complex task with reasoning and tool use
const result = await agent.process({
  text: "Analyze the latest AI research trends",
  useSearch: true,
  deepThink: true,
});
```

## Pattern Integration

### Pattern 2: Reusable Gemini Script
```typescript
// 02-reusable/gemini-run.ts
import { google } from '@ai-sdk/google';
import { generateText } from 'ai';

function getGeminiModel(modelName: string, thinkingLevel?: string) {
  const settings: any = {};

  if (thinkingLevel === 'high') {
    // Enable deep think mode
    settings.experimentalDeepThinking = true;
    settings.maxThinkingTokens = 50000;
  }

  return google(modelName, settings);
}

// Use in your script
const model = getGeminiModel('gemini-3.0-pro', 'high');
const result = await generateText({
  model,
  prompt: complexPrompt,
});
```

### Pattern 3: Gemini Sub-Agents
```typescript
// Vision Specialist Agent
const visionAgent = async (imageBuffer: Buffer, query: string) => {
  const model = genAI.getGenerativeModel({ model: "gemini-3.0-pro" });

  const result = await model.generateContent([
    {
      inlineData: {
        mimeType: 'image/png',
        data: imageBuffer.toString('base64'),
      },
    },
    { text: query },
  ]);

  return result.response.text();
};

// Audio Understanding Agent
const audioAgent = async (audioBuffer: Buffer) => {
  const model = genAI.getGenerativeModel({ model: "gemini-3.0-live" });

  const result = await model.generateContent([
    {
      inlineData: {
        mimeType: 'audio/wav',
        data: audioBuffer.toString('base64'),
      },
    },
    { text: "Transcribe and analyze the sentiment" },
  ]);

  return result.response.text();
};
```

### Pattern 4: MCP Wrapper with Gemini
```typescript
// Add to 04-mcp-wrapper/server.ts
const geminiAnalysisTool: ToolSchema = {
  name: "gemini_multimodal_analysis",
  description: "Analyze images, audio, or video with Gemini",
  inputSchema: {
    type: "object",
    properties: {
      mediaType: {
        type: "string",
        enum: ["image", "audio", "video"],
      },
      mediaData: {
        type: "string",
        description: "Base64 encoded media",
      },
      query: {
        type: "string",
        description: "What to analyze",
      },
      thinkingLevel: {
        type: "string",
        enum: ["low", "medium", "high"],
        default: "medium",
      },
    },
    required: ["mediaType", "mediaData", "query"],
  },
};
```

## Use Cases by Industry

### ðŸ¥ Healthcare
- Real-time medical image analysis during procedures
- Patient consultation with voice + visual symptoms
- Automated medical transcription with context

### ðŸ­ Manufacturing
- Live defect detection on production lines
- Visual quality control with explanations
- Real-time equipment monitoring

### ðŸ“š Education
- Interactive tutoring with screen sharing
- Live code review and explanation
- Language learning with pronunciation feedback

### ðŸŽ® Gaming
- Voice-controlled game assistants
- Real-time strategy advice
- Stream commentary generation

### ðŸ’¼ Enterprise
- Meeting transcription with visual context
- Document analysis during calls
- Live technical support with screen sharing

## Configuration Options

```typescript
interface GeminiLiveConfig {
  // Model selection
  model: 'gemini-3.0-live' | 'gemini-3.0-pro' | 'gemini-2.0-flash';

  // Reasoning depth
  thinkingLevel: 'low' | 'medium' | 'high';

  // Visual processing
  mediaResolution: 'low' | 'medium' | 'high';

  // Voice settings
  vadEnabled: boolean;
  interruptionThreshold: number; // 0-1

  // Features
  searchGrounding: boolean;
  codeExecution: boolean;
  functionCalling: boolean;

  // Session
  sessionMemory: boolean;
  contextWindow: number; // Up to 1M tokens
}
```

## Best Practices

### 1. Choose the Right Model
- **gemini-3.0-live**: Real-time multimodal (video calls, live assistance)
- **gemini-3.0-pro**: Complex reasoning (deep analysis, coding)
- **gemini-2.0-flash**: Fast responses (chat, simple tasks)

### 2. Optimize for Your Use Case
```typescript
// Real-time assistance (low latency priority)
const fastConfig = {
  model: 'gemini-3.0-live',
  thinkingLevel: 'low',
  mediaResolution: 'medium',
};

// Complex analysis (quality priority)
const deepConfig = {
  model: 'gemini-3.0-pro',
  thinkingLevel: 'high',
  mediaResolution: 'high',
};

// Balanced approach
const balancedConfig = {
  model: 'gemini-3.0-live',
  thinkingLevel: 'medium',
  mediaResolution: 'medium',
};
```

### 3. Handle Streaming Properly
```typescript
// Chunked response handling
client.on('chunk', (chunk) => {
  if (chunk.type === 'text') {
    process.stdout.write(chunk.content);
  } else if (chunk.type === 'audio') {
    audioQueue.push(chunk.data);
  }
});

// Error recovery
client.on('error', (error) => {
  console.error('Stream error:', error);
  // Implement reconnection logic
  reconnectWithBackoff();
});
```

## Cost Optimization

### Token Usage by Resolution
| Resolution | Image Tokens | Video Tokens/sec | Audio Tokens/sec |
|------------|-------------|------------------|------------------|
| Low        | ~250        | ~100             | ~50              |
| Medium     | ~750        | ~300             | ~50              |
| High       | ~2000       | ~800             | ~50              |

### Optimization Strategies
1. **Use Flash for simple tasks** - 10x cheaper than Pro
2. **Adjust media resolution dynamically** - Lower for overview, higher for details
3. **Implement caching** - Reuse analysis results
4. **Batch processing** - Group similar requests
5. **Use thinking levels wisely** - High only when needed

## Troubleshooting

### Common Issues

**WebSocket connection fails**
```typescript
// Add retry logic
const connectWithRetry = async (maxRetries = 3) => {
  for (let i = 0; i < maxRetries; i++) {
    try {
      await client.connect();
      break;
    } catch (error) {
      console.log(`Retry ${i + 1}/${maxRetries}`);
      await new Promise(r => setTimeout(r, 1000 * Math.pow(2, i)));
    }
  }
};
```

**Audio/Video sync issues**
```typescript
// Use timestamp alignment
const alignedStream = {
  timestamp: Date.now(),
  audio: audioBuffer,
  video: videoFrame,
};
```

**High latency**
- Reduce media resolution
- Use regional endpoints
- Enable edge caching
- Optimize network path

## Next Steps

1. **Try the examples** in `gemini-live/examples/`
2. **Build a prototype** with Pattern 2
3. **Scale to production** with Pattern 5
4. **Join the community** for Gemini best practices

## Resources
- [Google AI Studio](https://aistudio.google.com)
- [Gemini API Docs](https://ai.google.dev/docs)
- [Vertex AI](https://cloud.google.com/vertex-ai)
- [Gemini Cookbook](https://github.com/google-gemini/cookbook)